{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KZkddh6ojgpA"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"iMXRDFcMYDm2"},"source":["# EDA"]},{"cell_type":"markdown","metadata":{"id":"XQoN5ZFhp8aY"},"source":["### Dataset Constants & Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ6CV58wl5I5"},"outputs":[],"source":["TRAIN_DIR = '/content/drive/MyDrive/Final-project/Dataset/PlantDoc-Dataset/train/'\n","TEST_DIR = '/content/drive/MyDrive/Final-project/Dataset/PlantDoc-Dataset/test/'\n","IMAGE_SIZE = 224 # Image size of resize when applying transforms\n","NUM_WORKERS = 2 # Number of parallel processes for data preparation\n","\n","VALID_SPLIT = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.05}\n","train_batch_size = 32 # @param {type:\"integer\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccbE4tcUjlbA"},"outputs":[],"source":["# !unzip -u \"/content/drive/MyDrive/ai_ind_cv/ta/input/project-plantdoc-dataset.zip\" -d \"/content/drive/MyDrive/ai_ind_cv/ta/input_clear/\"\n","# Note:\n","# -u part controls extraction only if new/necessary. It is important if suddenly you lose connection or hardware switches off.\n","# -d creates the directory and extracted files are stored there."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0OFDRJJNArk"},"outputs":[],"source":["CLASS_NAMES = [\n","    'Apple Scab Leaf', 'Apple leaf', 'Apple rust leaf', 'Bell_pepper leaf',\n","    'Bell_pepper leaf spot', 'Blueberry leaf', 'Cherry leaf', 'Corn Gray leaf spot',\n","    'Corn leaf blight', 'Corn rust leaf', 'Peach leaf', 'Potato leaf early blight',\n","    'Potato leaf late blight', 'Raspberry leaf', 'Soyabean leaf',\n","    'Squash Powdery mildew leaf', 'Strawberry leaf', 'Tomato Early blight leaf',\n","    'Tomato Septoria leaf spot', 'Tomato leaf', 'Tomato leaf bacterial spot',\n","    'Tomato leaf late blight', 'Tomato leaf mosaic virus',\n","    'Tomato leaf yellow virus', 'Tomato mold leaf', 'grape leaf', 'grape leaf black rot'\n","]"]},{"cell_type":"markdown","metadata":{"id":"UKuahWXtY9_n"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rih5gKvxYFsD"},"outputs":[],"source":["import os\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import random\n","\n","plt.figure(figsize=(21,6))\n","for i in range(0, len(CLASS_NAMES)):\n","    plt.subplot(3,9, i+1)\n","    img_name = ''.join(random.sample(os.listdir(TRAIN_DIR+CLASS_NAMES[i]),1))\n","    img = mpimg.imread(os.path.join(TRAIN_DIR, CLASS_NAMES[i],img_name))\n","    label = CLASS_NAMES[i]\n","    plt.imshow(img)\n","    plt.xlabel(label, fontsize=8)\n","    plt.xticks([])\n","    plt.yticks([])"]},{"cell_type":"markdown","metadata":{"id":"CN_39QwEZRBE"},"source":["### Data Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffFs62qKZY05"},"outputs":[],"source":["import torchvision.transforms as T\n","\n","#gray_transformation = grayscale_transform = T.Grayscale(3)\n","resize_transformation = T.Resize((IMAGE_SIZE, IMAGE_SIZE))\n","hflip_transformation = T.RandomHorizontalFlip(p=1)\n","vflip_transformation = T.RandomVerticalFlip(p=1)\n","random_rotation_transformation = T.RandomRotation(35)\n","random_perspective_transformation = T.RandomPerspective(distortion_scale=0.25, p=1)\n","gausian_blur_transformation = T.GaussianBlur(kernel_size = 3, sigma = (0.5 , 1.5))\n","gausian_blur_transformation2 = T.GaussianBlur(kernel_size = (7,13), sigma = (5 , 8))\n","colour_jitter_transformation = T.ColorJitter(brightness=(0.4),\n","                                               contrast=(0.4),\n","                                               saturation=(0.4),\n","                                               hue=(0))\n","colour_jitter_transformation_2 = T.ColorJitter(brightness=(0.7),contrast=(6),saturation=(0.9),hue=(-0.1,0.1))\n","normalize_transformation = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225])])\n","sharpness_transformation = T.RandomAdjustSharpness(sharpness_factor=2, p=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8DtsxuvZaFt"},"outputs":[],"source":["from PIL import Image\n","\n","img_name = os.listdir(TRAIN_DIR+CLASS_NAMES[0])[5]\n","np_img = mpimg.imread(os.path.join(TRAIN_DIR, CLASS_NAMES[0],img_name))\n","\n","def trf_img(img):\n","    orig_img = Image.fromarray(np_img.astype('uint8'), 'RGB')\n","    #gray_img = grayscale_transform(orig_img)\n","    resize_img = resize_transformation(orig_img)\n","    h_img = hflip_transformation(orig_img)\n","    v_img = vflip_transformation(orig_img)\n","    rotation_img = random_rotation_transformation(orig_img)\n","    persp_img = random_perspective_transformation(orig_img)\n","    gausianblured_img = gausian_blur_transformation(orig_img)\n","    cjitter_img = colour_jitter_transformation(orig_img)\n","    normalize_img = normalize_transformation(orig_img)\n","    sharp_img = sharpness_transformation(orig_img)\n","    return [orig_img, resize_img, h_img, v_img, rotation_img,\n","            persp_img, gausianblured_img, cjitter_img, sharp_img]\n","\n","transform_img = trf_img(np_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TED8qvp7ZikJ"},"outputs":[],"source":["import matplotlib.gridspec as grd\n","\n","trf_name = ['Original Image', #'Grayscale Image',\n","            'Resize Image',\n","            'Horizontal Flip Image', 'Vertical Flip Image',\n","            'Rotation Image', 'Perspective Image',\n","            'Gaussian Blurred Image','Color Jitter Image', 'Sharpen Image']\n","fig = plt.figure(figsize=(16, 10))\n","gs = grd.GridSpec(3, 4, wspace=0.1, hspace=0.2)\n","ax0= fig.add_subplot(gs[0, :])\n","ax0.imshow(transform_img[0])\n","ax0.set_title(trf_name[0])\n","ax0.set_xticks([])\n","ax0.set_yticks([])\n","# for i in range(len(transform_img)):\n","#   if i == 0:\n","#     ax0= fig.add_subplot(gs[0, :])\n","#     ax0.imshow(transform_img[0])\n","#     ax0.set_title(trf_name[i])\n","#     ax0.set_xticks([])\n","#     ax0.set_yticks([])\n","#   else:\n","#     ax[i]= fig.add_subplot(gs[1, i-1])\n","#     ax[i].imshow(transform_img[i])\n","#     ax[i].set_xticks([])\n","#     ax[i].set_yticks([])\n","\n","ax1= fig.add_subplot(gs[1, 0])\n","ax1.set_title(trf_name[1])\n","ax1.imshow(transform_img[1])\n","ax1.set_xticks([])\n","ax1.set_yticks([])\n","\n","ax2= fig.add_subplot(gs[1, 1])\n","ax2.set_title(trf_name[2])\n","ax2.imshow(transform_img[2])\n","ax2.set_xticks([])\n","ax2.set_yticks([])\n","\n","ax3= fig.add_subplot(gs[1, 2])\n","ax3.set_title(trf_name[3])\n","ax3.imshow(transform_img[3])\n","ax3.set_xticks([])\n","ax3.set_yticks([])\n","\n","ax4= fig.add_subplot(gs[1, 3])\n","ax4.set_title(trf_name[4])\n","ax4.imshow(transform_img[4])\n","ax4.set_xticks([])\n","ax4.set_yticks([])\n","\n","ax5= fig.add_subplot(gs[2, 0])\n","ax5.set_title(trf_name[5])\n","ax5.imshow(transform_img[5])\n","ax5.set_xticks([])\n","ax5.set_yticks([])\n","\n","ax6= fig.add_subplot(gs[2, 1])\n","ax6.set_title(trf_name[6])\n","ax6.imshow(transform_img[6])\n","ax6.set_xticks([])\n","ax6.set_yticks([])\n","\n","ax7= fig.add_subplot(gs[2, 2])\n","ax7.set_title(trf_name[7])\n","ax7.imshow(transform_img[7])\n","ax7.set_xticks([])\n","ax7.set_yticks([])\n","\n","ax8= fig.add_subplot(gs[2, 3])\n","ax8.set_title(trf_name[8])\n","ax8.imshow(transform_img[8])\n","ax8.set_xticks([])\n","ax8.set_yticks([])"]},{"cell_type":"markdown","metadata":{"id":"xWVgNk-YZpKp"},"source":["### Image with Noise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvWU7juMZjO5"},"outputs":[],"source":["# img_name = ['Bell_pepper leaf spot (3).jpg', 'Cherry leaf (45).jpg', 'Corn Gray leaf spot (27).jpg', 'Tomato Septoria leaf spot (3).jpg']\n","# plt.figure(figsize=(14,6))\n","# for idx, name in enumerate(img_name):\n","#     plt.subplot(1, 4, idx+1 )\n","#     folder_name = name.split(' (')[0]\n","#     img_name = name\n","#     img = mpimg.imread(os.path.join(TRAIN_DIR, folder_name,img_name))\n","#     plt.imshow(img)\n","#     plt.xticks([])\n","#     plt.yticks([])\n","# plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"-MaXRYgBp-iU"},"source":["### Data Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bPSzMT7p9wT"},"outputs":[],"source":["import pandas as pd\n","\n","plants = os.listdir(TRAIN_DIR)\n","\n","class_counts = {}\n","for plant in plants:\n","    class_counts[plant] = len(os.listdir(os.path.join(TRAIN_DIR, plant)))\n","\n","# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n","img_per_class = pd.DataFrame(class_counts.values(), index=class_counts.keys(), columns=[\"no. of images\"])\n","img_per_class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0t2-Eh5cq-Q6"},"outputs":[],"source":["# plotting number of images available for each disease\n","index = [n for n in range(len(plants))]\n","plt.figure(figsize=(20, 5))\n","plt.bar(index, [n for n in class_counts.values()], width=0.3)\n","plt.xlabel('Plants/Disease', fontsize=10)\n","plt.ylabel('No of images available', fontsize=10)\n","plt.xticks(index, plants, fontsize=5, rotation=90)\n","plt.title('Images per each class of plant/disease')"]},{"cell_type":"markdown","metadata":{"id":"g0-lOn6zq2LM"},"source":["# Data Pre-processing"]},{"cell_type":"markdown","source":["### Data Cleaning"],"metadata":{"id":"XnSqagkYLe_9"}},{"cell_type":"code","source":["# # menghapus data \"kotor\" atau data yang tidak valid (yaitu gambar dengan lebih dari satu data kelas)\n","# train_del = {\n","#     'Bell_pepper leaf spot' : ['Bell_pepper leaf spot (3).jpg', 'Bell_pepper leaf spot (4).jpg'],\n","#     'Cherry leaf' : ['Cherry leaf (45).jpg'],\n","#     'Corn Gray leaf spot': ['Corn Gray leaf spot (24).jpg', 'Corn Gray leaf spot (27).jpg', 'Corn Gray leaf spot (31).jpg'],\n","#     'Corn leaf blight' : ['Corn leaf blight (49).jpg', 'Corn leaf blight (99).jpg', 'Corn leaf blight (164).jpg'],\n","#     'Strawberry leaf' : ['Strawberry leaf (6).jpg'],\n","#     'Tomato leaf bacterial spot': ['Tomato leaf bacterial spot (18).jpg', 'Tomato leaf bacterial spot (44).jpg', 'Tomato leaf bacterial spot (45).jpg'],\n","#     'Tomato leaf yellow virus' : ['Tomato leaf yellow virus (51).jpg', 'Tomato leaf yellow virus (63).jpg'],\n","#     'Tomato Septoria leaf spot' : ['Tomato Septoria leaf spot (3).jpg', 'Tomato Septoria leaf spot (111).jpg', 'Tomato Septoria leaf spot (115).jpg'],\n","# }\n","\n","# test_del = {\n","#     'Bell_pepper leaf spot' : ['Bell_pepper leaf spot (3).jpg'],\n","#     'Tomato Early blight leaf' : ['Tomato Early blight leaf (1).jpg'],\n","#     'Tomato leaf bacterial spot' : ['Tomato leaf bacterial spot (1).jpg'],\n","#     'Tomato leaf mosaic virus' : ['Tomato leaf mosaic virus (5).jpg'],\n","#     'Tomato leaf yellow virus' : ['Tomato leaf yellow virus (6).jpg'],\n","#     'Tomato mold leaf' : ['Tomato mold leaf (3).jpg'],\n","#     'Tomato Septoria leaf spot' : ['Tomato Septoria leaf spot (1).jpg']\n","# }\n","\n","# for k in train_del:\n","#     for v in train_del[k]:\n","#         filename = os.path.join(TRAIN_DIR, k, v)\n","#         !rm -rf \"$filename\"\n","\n","# for k in test_del:\n","#     for v in test_del[k]:\n","#         filename = os.path.join(TEST_DIR, k, v)\n","#         !rm -rf \"$filename\""],"metadata":{"id":"iNfRB5PmLc2L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Augmentations"],"metadata":{"id":"tkY_C-4ELTJu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnRiFCUzqwiA"},"outputs":[],"source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","def get_train_transform(image_size):\n","    train_transform = transforms.Compose([\n","        transforms.Resize((image_size, image_size)),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomVerticalFlip(p=0.5),\n","        transforms.RandomRotation(35),\n","        transforms.RandomPerspective(distortion_scale=0.25, p=0.5),\n","        # transforms.CenterCrop((192, 192)),\n","        transforms.ColorJitter(brightness=0.4,\n","                               contrast=0.4,\n","                               saturation=0.4,\n","                               hue=0),\n","        transforms.GaussianBlur(kernel_size=3, sigma=(0.5, 1.5)),\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","            )\n","    ])\n","    return train_transform\n","\n","def get_valid_transform(image_size):\n","    valid_transform = transforms.Compose([\n","        transforms.Resize((image_size, image_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","            )\n","    ])\n","    return valid_transform"]},{"cell_type":"markdown","source":["### Class Weighting"],"metadata":{"id":"uTJTjnn8c4pu"}},{"cell_type":"code","source":["class_weights = [1./n for n in class_counts.values()]\n","class_weights = torch.tensor(class_weights)\n","class_weights"],"metadata":{"id":"tcF1ldCh5jNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ey2wg8-Xkog"},"source":["#Dataset"]},{"cell_type":"markdown","metadata":{"id":"Wba8K0YzrEM6"},"source":["### Datasets and DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zn--Wc0XnaN"},"outputs":[],"source":["def get_train_datasets():\n","    \"\"\"\n","    Images with the same class is already grouped in its class folder.\n","    Therefore, datasets can be generated directly from the ImageFolder function of Dataset class.\n","\n","    References:\n","    - https://pytorch.org/vision/stable/datasets.html\n","    - https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html\n","    \"\"\"\n","\n","    dataset = datasets.ImageFolder(\n","        TRAIN_DIR,\n","        transform=(get_train_transform(IMAGE_SIZE))\n","    )\n","    dataset_test = datasets.ImageFolder(\n","        TRAIN_DIR,\n","        transform=(get_valid_transform(IMAGE_SIZE))\n","    )\n","\n","    # split training dataset to training and validation dataset\n","    dataset_size = len(dataset)\n","    # Calculate the validation dataset size\n","    valid_size = int(VALID_SPLIT * dataset_size)\n","    # Radomize the data indices\n","    indices = torch.randperm(len(dataset)).tolist()\n","    # Training and validation sets\n","    dataset_train = Subset(dataset, indices[:-valid_size])\n","    dataset_valid = Subset(dataset_test, indices[-valid_size:])\n","\n","    return dataset_train, dataset_valid, dataset.classes\n","\n","def get_train_data_loaders(dataset_train, dataset_valid, batch_size):\n","    train_loader = DataLoader(\n","        dataset_train, batch_size=batch_size,\n","        shuffle=True, num_workers=NUM_WORKERS\n","    )\n","    valid_loader = DataLoader(\n","        dataset_valid, batch_size=batch_size,\n","        shuffle=False, num_workers=NUM_WORKERS\n","    )\n","\n","    return train_loader, valid_loader"]},{"cell_type":"markdown","metadata":{"id":"tyAPiaNP4USE"},"source":["### Load Data for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ELG6cwY4Mlo"},"outputs":[],"source":["# Load datasets.\n","dataset_train, dataset_valid, dataset_classes = get_train_datasets()\n","\n","# Load data loaders.\n","train_loader, valid_loader = get_train_data_loaders(\n","    dataset_train, dataset_valid, batch_size=train_batch_size\n",")\n","\n","print(f\"[DATASET] Training data: {len(dataset_train)} images\")\n","print(f\"[DATASET] Validation data: {len(dataset_valid)} images\\n\")\n","print(f\"[DATASET] Class count: {len(dataset_classes)}\")\n","print(f\"[DATASET] Class names: {dataset_classes}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"csX5gGkfXKDA"},"source":["# Model"]},{"cell_type":"markdown","source":["### Parameters"],"metadata":{"id":"1tS9dYHzCby8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H92wINPNpjy6"},"outputs":[],"source":["model_name = \"vgg19\" # @param [\"resnet50\", \"resnet152\", \"wide_resnet50\", \"vgg19\"]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PE9plJs6KRWn"},"outputs":[],"source":["# dropout setting for final layer\n","dropout = 0.15 # @param {type:\"slider\", min:0, max:1, step:0.025}\n","\n","# Transfer learning setting\n","fine_tune = True\n","# True for fine-tuning all layers\n","# False for freezing hidden layers\n","\n","# Device\n","device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(f\"Dropout: {dropout}\")\n","print(f\"Transfer learning: {fine_tune}\")\n","print(f\"Computation device: {device}\")"]},{"cell_type":"markdown","source":["### Architecture"],"metadata":{"id":"nZjx3-3AClIa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KV75--UrO1Fq"},"outputs":[],"source":["from torchvision import models\n","\n","import torch.nn as nn\n","\n","def build_model(model_name='resnet50', fine_tune=True, dropout=0, num_classes=27):\n","    models_list = {\n","        'resnet50': models.resnet50(weights='DEFAULT'),\n","        'wide_resnet50': models.wide_resnet50_2(weights='DEFAULT'),\n","        'vgg19': models.vgg19(weights='DEFAULT'),\n","    }\n","\n","    model = models_list[model_name]\n","\n","    if fine_tune:\n","        print('[INFO]: Fine-tuning all layers...')\n","        for params in model.parameters():\n","            params.requires_grad = True\n","    else:\n","        print('[INFO]: Freezing hidden layers...')\n","        for params in model.parameters():\n","            params.requires_grad = False\n","\n","    if 'resnet' in model_name:\n","        # Replace the last fully-connected layer to adapt the model to a new problem with a different number of output classes\n","        num_ftrs = model.fc.in_features\n","        # model.fc = nn.Linear(in_features=num_ftrs, out_features=num_classes) # model.fc = nn.Linear(in_features=2048, out_features=num_classes)\n","        if dropout > 0:\n","            model.fc = nn.Sequential(\n","                nn.Dropout(dropout),\n","                nn.Linear(in_features=num_ftrs, out_features=num_classes)\n","            )\n","        else:\n","            model.fc = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n","        # https://discuss.pytorch.org/t/resnet-last-layer-modification/33530/2\n","\n","    elif model_name == 'vgg19':\n","        last_item_index = len(model.classifier)-1\n","        old_fc = model.classifier.__getitem__(last_item_index )\n","        if dropout > 0:\n","            new_fc = nn.Sequential(\n","                nn.Dropout(dropout),\n","                nn.Linear(in_features=old_fc.in_features, out_features= num_classes, bias=True)\n","            )\n","        else:\n","            new_fc = nn.Linear(in_features=old_fc.in_features, out_features= num_classes, bias=True)\n","        model.classifier.__setitem__(last_item_index , new_fc)\n","\n","    return model"]},{"cell_type":"markdown","source":["### Load the Model"],"metadata":{"id":"VDyHHM4HCvuj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_un9yn2thwI"},"outputs":[],"source":["model = build_model(model_name, fine_tune=fine_tune, dropout=dropout, num_classes=len(dataset_classes)).to(device)\n","\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmsmUzjAECPW"},"outputs":[],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSA4cVHvDYzI"},"outputs":[],"source":["from torchinfo import summary\n","summary(model, input_size=(train_batch_size, 3, IMAGE_SIZE, IMAGE_SIZE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ON0GhYB7u4Vs"},"outputs":[],"source":["# Total parameters and trainable parameters.\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"[MODEL] {total_params:,} total parameters.\")\n","\n","total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"[MODEL] {total_trainable_params:,} trainable parameters.\")"]},{"cell_type":"markdown","metadata":{"id":"nUxuZ3cgYKro"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"aztKz8g2KXME"},"source":["### Directory for Outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTN_V7li1H0B"},"outputs":[],"source":["from datetime import datetime\n","import pytz\n","\n","tz_jkt = pytz.timezone('Asia/Jakarta')\n","now_jkt = datetime.now(tz_jkt)\n","dt_str = now_jkt.strftime(\"%y-%m-%d_%H.%M\")\n","\n","# Create a directory with the model name for outputs.\n","out_dir = os.path.join(f'/content/drive/MyDrive/Final-project/outputs/{dt_str}', model_name)\n","\n","os.makedirs(out_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"vHjR0zaQyBHg"},"source":["### Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YQMvYYXXYO4"},"outputs":[],"source":["matplotlib.style.use('ggplot')\n","\n","class SaveBestModel:\n","    \"\"\"\n","    Class to save the best model while training. If the current epoch's\n","    validation loss is less than the previous least less, then save the\n","    model state.\n","    \"\"\"\n","    def __init__(self, best_valid_loss=float('inf'), best_valid_acc=float('inf'), best_train_loss=float('inf'), best_train_acc=float('inf'), best_epoch=0):\n","        self.best_valid_loss = best_valid_loss\n","        self.best_valid_acc = best_valid_acc\n","        self.best_train_loss = best_train_loss\n","        self.best_train_acc = best_train_acc\n","        self.best_epoch = best_epoch\n","\n","    def __call__(self, current_valid_loss, current_valid_acc, current_train_loss, current_train_acc, epoch, model, out_dir):\n","        if current_valid_loss < self.best_valid_loss:\n","            self.best_valid_loss = current_valid_loss\n","            self.best_valid_acc = current_valid_acc\n","            self.best_train_loss = current_train_loss\n","            self.best_train_acc = current_train_acc\n","            self.best_epoch = epoch\n","            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n","            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n","            torch.save({\n","                'epoch': epoch+1,\n","                'model_state_dict': model.state_dict(),\n","                }, os.path.join(out_dir, 'best_model.pth'))\n","\n","def save_model(epochs, model, optimizer, criterion, out_dir):\n","    \"\"\"\n","    Function to save the trained model to disk.\n","    \"\"\"\n","    torch.save({\n","                'epoch': epochs,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': criterion,\n","                }, os.path.join(out_dir, 'model.pth'))\n","\n","def save_plots(train_acc, valid_acc, train_loss, valid_loss, out_dir):\n","    \"\"\"\n","    Function to save the loss and accuracy plots to disk.\n","    \"\"\"\n","    # Accuracy plots.\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(\n","        train_acc, color='tab:blue', linestyle='-',\n","        label='train accuracy'\n","    )\n","    plt.plot(\n","        valid_acc, color='tab:red', linestyle='-',\n","        label='validation accuracy'\n","    )\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.savefig(os.path.join(out_dir, 'accuracy.png'))\n","\n","    # Loss plots.\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(\n","        train_loss, color='tab:blue', linestyle='-',\n","        label='train loss'\n","    )\n","    plt.plot(\n","        valid_loss, color='tab:red', linestyle='-',\n","        label='validation loss'\n","    )\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig(os.path.join(out_dir, 'loss.png'))"]},{"cell_type":"markdown","metadata":{"id":"UkhRdIQa0uTp"},"source":["### Training & Validation Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OxUooXWY74G"},"outputs":[],"source":["import torch.optim as optim\n","\n","from tqdm.auto import tqdm\n","\n","seed = 42\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True\n","\n","# Training function.\n","def train(model, trainloader, optimizer, criterion, scheduler=None): # def train(model, trainloader, optimizer, criterion, scheduler = None):\n","    model.train()\n","    print('Training')\n","    train_running_loss = 0.0\n","    train_running_correct = 0\n","    counter = 0\n","    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n","        counter += 1\n","        image, labels = data\n","        image = image.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        # Forward pass.\n","        outputs = model(image)\n","        # Calculate the loss.\n","        loss = criterion(outputs, labels)\n","        train_running_loss += loss.item()\n","        # Calculate the accuracy.\n","        _, preds = torch.max(outputs.data, 1)\n","        train_running_correct += (preds == labels).sum().item()\n","        # Backpropagation.\n","        loss.backward()\n","        # Update the weights.\n","        optimizer.step()\n","\n","        # Update learning rate melalui scheduler\n","        # scheduler.step() dipanggil setelah optimizer.step() dipanggil terlebih dahulu\n","        if scheduler is not None:\n","            scheduler.step()\n","            # print(scheduler.get_last_lr())\n","\n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = train_running_loss / counter\n","    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n","    return epoch_loss, epoch_acc\n","\n","# Validation function.\n","def validate(model, testloader, criterion, class_names):\n","    model.eval()\n","    print('Validation')\n","    valid_running_loss = 0.0\n","    valid_running_correct = 0\n","    counter = 0\n","\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n","            counter += 1\n","\n","            image, labels = data\n","            image = image.to(device)\n","            labels = labels.to(device)\n","            # Forward pass.\n","            outputs = model(image)\n","            # Calculate the loss.\n","            loss = criterion(outputs, labels)\n","            valid_running_loss += loss.item()\n","            # Calculate the accuracy.\n","            _, preds = torch.max(outputs.data, 1)\n","            valid_running_correct += (preds == labels).sum().item()\n","\n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = valid_running_loss / counter\n","    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n","    return epoch_loss, epoch_acc"]},{"cell_type":"markdown","metadata":{"id":"1Ttby-yDiXp_"},"source":["### Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aD3rWDA8aWke"},"outputs":[],"source":["epochs = 30 # @param {type:\"integer\"}\n","\n","learning_rate = 0.001 # @param {type:\"number\"}\n","\n","opt_type = \"SGD\" # @param [\"SGD\", \"Adam\"]\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) if opt_type == 'SGD' else optim.Adam(params=model.parameters(), lr=learning_rate)\n","\n","\"\"\"\n","The most common hyperparameters for the Adam optimizer\n","- params (iterable) – iterable of parameters to optimize or dicts defining parameter groups\n","- Learning rate (lr): controls the step size at each iteration during gradient descent. Default: 1e-3\n","  A more significant learning rate can help the optimizer converge faster but may also cause it to overshoot the optimal solution.\n","- Beta1 (beta_1): controls the exponential decay rate for the first moment estimates. Default: 0.9\n","  It is typically set to 0.9 but can be adjusted to a trade-off between stability and responsiveness.\n","- Beta2 (beta_2): controls the exponential decay rate for the second-moment estimates. Default: 0.999\n","  It is typically set to 0.999 but can be adjusted to a trade-off between stability and responsiveness.\n","- Epsilon (epsilon): a small constant added to the denominator to avoid division by zero. Default: 1e-08\n","- Decay (decay): controls the learning rate decay over time.\n","  It is typically set to 0, meaning the learning rate remains constant.\n","\n","References:\n","https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n","https://www.analyticsvidhya.com/blog/2023/12/adam-optimizer/\n","https://www.kdnuggets.com/2022/12/tuning-adam-optimizer-parameters-pytorch.html\n","https://spotintelligence.com/2023/03/01/adam-optimizer/\n","\"\"\"\n","\n","print(f\"[PARAMETER] Epochs: {epochs}\")\n","print(f\"[PARAMETER] Learning rate: {learning_rate}\\n\")\n","print(f\"[PARAMETER] Optimizer: {optimizer}\\n\")"]},{"cell_type":"code","source":["# Scheduler\n","\n","use_scheduler = False # @param {type:\"boolean\"}\n","\n","# if use StepLR:\n","epoch_to_update = 2 # @param {type:\"integer\"}\n","step_size = epoch_to_update * len(dataset_train)/train_batch_size\n","\n","gamma = 0.9 # @param {type:\"number\"}\n","\n","# if use OneCycleLR:\n","# max_learning_rate = 0.01 # @param {type:\"number\"}\n","\n","scheduler = None\n","if use_scheduler:\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","    # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","    #    optimizer, max_lr=max_learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n"],"metadata":{"id":"Gd8fL0wibSSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTDDEmfJJ1kt"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmTOIM1mZGr8"},"outputs":[],"source":["# Loss function\n","criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), reduction='mean') # class weights added here\n","\n","# Initialize 'SaveBestModel' class.\n","save_best_model = SaveBestModel()\n","\n","# Lists to keep track of losses and accuracies.\n","train_loss, valid_loss = [], []\n","train_acc, valid_acc = [], []\n","\n","# Start the training.\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss, train_epoch_acc = train(model, train_loader,\n","                                              optimizer, criterion, scheduler)\n","    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,\n","                                                 criterion, dataset_classes)\n","\n","    # Note:\n","    # Selain dalam bagian def train(), scheduler bisa juga ditempatkan di sini\n","    # if scheduler is not None:\n","        # scheduler.step()\n","        # print(f'\\nLR: {scheduler.get_last_lr()}\\n') #get current learning rate\n","\n","    train_loss.append(train_epoch_loss)\n","    valid_loss.append(valid_epoch_loss)\n","    train_acc.append(train_epoch_acc)\n","    valid_acc.append(valid_epoch_acc)\n","    print(f\"[TRAINING] loss: {train_epoch_loss:.3f}, acc: {train_epoch_acc:.3f}\")\n","    print(f\"[VALIDATION] loss: {valid_epoch_loss:.3f}, acc: {valid_epoch_acc:.3f}\")\n","    save_best_model(valid_epoch_loss, valid_epoch_acc, train_epoch_loss, train_epoch_acc, epoch, model, out_dir)\n","    print('-'*50 + '\\n')\n","\n","print('TRAINING COMPLETE')\n","print(f\"[EPOCH] best epoch: {save_best_model.best_epoch}\")\n","print(f\"[TRAINING] loss: {save_best_model.best_train_loss:.3f}, acc: {save_best_model.best_train_acc:.3f}\")\n","print(f\"[VALIDATION] loss: {save_best_model.best_valid_loss:.3f}, acc: {save_best_model.best_valid_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"g3fRywfh7KcG"},"source":["### Saving Result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Cu0hxde7EWj"},"outputs":[],"source":["# Save the trained model weights.\n","save_model(epochs, model, optimizer, criterion, out_dir)\n","# Save the loss and accuracy plots.\n","save_plots(train_acc, valid_acc, train_loss, valid_loss, out_dir)"]},{"cell_type":"markdown","metadata":{"id":"9v6fFqSyNWma"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIH4tjQ1PZ75"},"outputs":[],"source":["# check current folder & model\n","print(dt_str, model_name)"]},{"cell_type":"code","source":["\"\"\"\n","############################################################################\n","# Jika diperlukan untuk menjalankan testing manual (tanpa menjalankan proses training di atas)...\n","# Un-comment cell ini\n","# dan atur nilai 'dt_str' dan 'model_name'\n","# sesuai dengan folder yang akan dijalankan\n","############################################################################\n","\n","dt_str = '24-02-07_04.53' # ubah folder target di sini\n","model_name = 'resnet152' # sesuaikan nama modelnya\n","\n","import torch\n","import os\n","import torch.nn as nn\n","from torchvision import models\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","from tqdm.auto import tqdm\n","\n","TEST_DIR = '/content/drive/MyDrive/ai_ind_cv/ta/input_clear/PlantDoc-Dataset/test/'\n","IMAGE_SIZE = 224\n","NUM_WORKERS = 2\n","\n","CLASS_NAMES = [\n","    'Apple Scab Leaf', 'Apple leaf', 'Apple rust leaf', 'Bell_pepper leaf',\n","    'Bell_pepper leaf spot', 'Blueberry leaf', 'Cherry leaf', 'Corn Gray leaf spot',\n","    'Corn leaf blight', 'Corn rust leaf', 'Peach leaf', 'Potato leaf early blight',\n","    'Potato leaf late blight', 'Raspberry leaf', 'Soyabean leaf',\n","    'Squash Powdery mildew leaf', 'Strawberry leaf', 'Tomato Early blight leaf',\n","    'Tomato Septoria leaf spot', 'Tomato leaf', 'Tomato leaf bacterial spot',\n","    'Tomato leaf late blight', 'Tomato leaf mosaic virus',\n","    'Tomato leaf yellow virus', 'Tomato mold leaf', 'grape leaf', 'grape leaf black rot'\n","]\n","\n","def build_model(model_name='resnet50', fine_tune=True, dropout=0, num_classes=27):\n","    models_list = {\n","        'resnet50': models.resnet50(weights='DEFAULT'),\n","        'wide_resnet50': models.wide_resnet50_2(weights='DEFAULT'),\n","    }\n","\n","    model = models_list[model_name]\n","\n","    if fine_tune:\n","        print('[INFO]: Fine-tuning all layers...')\n","        for params in model.parameters():\n","            params.requires_grad = True\n","    else:\n","        print('[INFO]: Freezing hidden layers...')\n","        for params in model.parameters():\n","            params.requires_grad = False\n","\n","    if 'resnet' in model_name:\n","        # Replace the last fully-connected layer to adapt the model to a new problem with a different number of output classes\n","        num_ftrs = model.fc.in_features\n","        # model.fc = nn.Linear(in_features=num_ftrs, out_features=num_classes) # model.fc = nn.Linear(in_features=2048, out_features=num_classes)\n","        if dropout > 0:\n","            model.fc = nn.Sequential(\n","                nn.Dropout(dropout),\n","                nn.Linear(in_features=num_ftrs, out_features=num_classes)\n","            )\n","        else:\n","            model.fc = nn.Linear(in_features=num_ftrs, out_features=num_classes)\n","        # https://discuss.pytorch.org/t/resnet-last-layer-modification/33530/2\n","\n","    elif model_name == 'vgg19':\n","        last_item_index = len(model.classifier)-1\n","        old_fc = model.classifier.__getitem__(last_item_index )\n","        if dropout > 0:\n","            new_fc = nn.Sequential(\n","                nn.Dropout(dropout),\n","                nn.Linear(in_features=old_fc.in_features, out_features= num_classes, bias=True)\n","            )\n","        else:\n","            new_fc = nn.Linear(in_features=old_fc.in_features, out_features= num_classes, bias=True)\n","        model.classifier.__setitem__(last_item_index , new_fc)\n","\n","    return model\n","\"\"\""],"metadata":{"id":"RbUjEEL3pSxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsEsiq_TkMOc"},"source":["### Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cTAjmGJNrWp"},"outputs":[],"source":["import pathlib\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","best_model_weights = f'/content/drive/MyDrive/Final-project/outputs/{dt_str}/{model_name}/best_model.pth'\n","weights_path = pathlib.Path(best_model_weights)\n","checkpoint = torch.load(weights_path)"]},{"cell_type":"markdown","metadata":{"id":"o6cUb_0FOPoY"},"source":["### Directory for Outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDFf781H9Fea"},"outputs":[],"source":["#model_name = str(weights_path).split(os.path.sep)[-2]\n","\n","test_result_save_dir = os.path.join(\n","    f'/content/drive/MyDrive/Final-project/outputs/{dt_str}/test_results', model_name\n",")\n","os.makedirs(test_result_save_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"WjiWIjrn0Z_6"},"source":["### Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mlsv66N1Aeea"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","def denormalize(\n","    x,\n","    mean=[0.485, 0.456, 0.406],\n","    std=[0.229, 0.224, 0.225]\n","):\n","    for t, m, s in zip(x, mean, std):\n","        t.mul_(s).add_(m)\n","    return torch.clamp(x, 0, 1)\n","\n","def save_test_results(\n","    tensor,\n","    target,\n","    output_class,\n","    counter,\n","    test_result_save_dir\n","):\n","    \"\"\"\n","    This function will save a few test images along with the\n","    ground truth label and predicted label annotated on the image.\n","\n","    :param tensor: The image tensor.\n","    :param target: The ground truth class number.\n","    :param output_class: The predicted class number.\n","    :param counter: The test image number.\n","    \"\"\"\n","    image = denormalize(tensor).cpu()\n","    image = image.squeeze(0).permute((1, 2, 0)).numpy()\n","    image = np.ascontiguousarray(image, dtype=np.float32)\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    gt = target.cpu().numpy()\n","\n","    # Enlarge the image a bit to accomodate for the large class names.\n","    image = cv2.resize(image, (384, 384))\n","    cv2.putText(\n","        image, f\"GT: {CLASS_NAMES[int(gt)]}\",\n","        (5, 25),\n","        cv2.FONT_HERSHEY_SIMPLEX,\n","        0.8,\n","        (0, 255, 0),\n","        2,\n","        cv2.LINE_AA\n","    )\n","    if output_class == gt:\n","        color = (0, 255, 0)\n","    else:\n","        color = (0, 0, 255)\n","    cv2.putText(\n","        image, f\"Pred: {CLASS_NAMES[int(output_class)]}\",\n","        (5, 55),\n","        cv2.FONT_HERSHEY_SIMPLEX,\n","        0.8,\n","        color,\n","        2,\n","        cv2.LINE_AA\n","    )\n","    cv2.imwrite(\n","        os.path.join(test_result_save_dir, 'test_image_'+str(counter)+'.png'),\n","        image*255.\n","    )"]},{"cell_type":"markdown","source":["### DataTransformation"],"metadata":{"id":"PGvgxwi6r59J"}},{"cell_type":"code","source":["def get_test_transform(image_size):\n","    test_transform = transforms.Compose([\n","        transforms.Resize((image_size, image_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","            )\n","    ])\n","    return test_transform"],"metadata":{"id":"pmPcy3mXr9Fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Datasets"],"metadata":{"id":"8r5cZiRCqUHk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJpwt-ACAVKk"},"outputs":[],"source":["test_batch_size = 1\n","\n","def get_test_datasets(image_size):\n","    dataset_test = datasets.ImageFolder(\n","        TEST_DIR,\n","        transform=(get_test_transform(image_size))\n","    )\n","    return dataset_test\n","\n","def get_test_data_loader(dataset_test):\n","    test_loader = DataLoader(\n","        dataset_test, batch_size=test_batch_size,\n","        shuffle=False, num_workers=NUM_WORKERS\n","    )\n","    return test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGCHdGD8LoxR"},"outputs":[],"source":["dataset_test = get_test_datasets(IMAGE_SIZE)\n","test_loader = get_test_data_loader(dataset_test)\n","\n","print(f\"[DATASET] Testing data: {len(dataset_test)} images\")"]},{"cell_type":"markdown","metadata":{"id":"dD_HRtL41YXL"},"source":["### Load the Model"]},{"cell_type":"code","source":["# check folder & model\n","print(dt_str, model_name)"],"metadata":{"id":"gMVFj4BysXa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_model_weights = f'/content/drive/MyDrive/Final-project/outputs/{dt_str}/{model_name}/best_model.pth'\n","weights_path = pathlib.Path(best_model_weights)\n","checkpoint = torch.load(weights_path)"],"metadata":{"id":"t8ZIhFQvk3Jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWuPMBodBwkF"},"outputs":[],"source":["load_model = build_model(model_name, fine_tune=False, dropout=dropout, num_classes=len(CLASS_NAMES)).to(DEVICE)\n","load_model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"I96XE5gl0krO"},"source":["### Test Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueziaNRR0nuh"},"outputs":[],"source":["def test(model, testloader, device, test_result_save_dir):\n","    \"\"\"\n","    Returns:\n","        predictions_list: List containing all the predicted class numbers.\n","        ground_truth_list: List containing all the ground truth class numbers.\n","        acc: The test accuracy.\n","    \"\"\"\n","\n","    model.eval()\n","    \"\"\"\n","    References: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","    Call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference.\n","    Failing to do this will yield inconsistent inference results.\n","    ----------------------------------------------------------------------------\n","    References: https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n","    model.eval()\n","    Sets model in evaluation (inference) mode:\n","    • normalisation layers use running statistics\n","    • de-activates Dropout layers\n","    Equivalent to model.train(False)\n","\n","    model.train()\n","    Sets model in training mode:\n","    • normalisation layers1 use per-batch statistics\n","    • activates Dropout layers2\n","    \"\"\"\n","\n","    print('Testing model')\n","    predictions_list = []\n","    ground_truth_list = []\n","    test_running_correct = 0\n","    counter = 0\n","\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n","            counter += 1\n","            image, labels = data\n","            image = image.to(device)\n","            labels = labels.to(device)\n","            # Forward pass.\n","            outputs = model(image)\n","            # Append the GT and predictions to the respective lists.\n","            ground_truth_list.append(labels.cpu().numpy())\n","            # Calculate the accuracy.\n","            _, preds = torch.max(outputs, 1)\n","            test_running_correct += (preds == labels).sum().item()\n","            predictions_list.append(preds.cpu().numpy())\n","            save_test_results(\n","                image,\n","                labels,\n","                preds.cpu().numpy(),\n","                counter,\n","                test_result_save_dir\n","            )\n","\n","    acc = 100. * (test_running_correct / len(testloader.dataset))\n","    return predictions_list, ground_truth_list, acc"]},{"cell_type":"markdown","metadata":{"id":"UDYxUkdT1cmW"},"source":["### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSTh62bi1am4"},"outputs":[],"source":["predictions_list, ground_truth_list, acc = test(\n","    load_model,\n","    test_loader,\n","    DEVICE,\n","    test_result_save_dir\n",")\n","print(f\"[TESTING] accuracy: {acc:.3f}%\")"]},{"cell_type":"markdown","metadata":{"id":"F5CuovABfjsu"},"source":["# Inferences"]},{"cell_type":"code","source":["#model_name = str(weights_path).split(os.path.sep)[-2]\n","\n","infer_result_path = os.path.join(\n","    f'/content/drive/MyDrive/Final-project/outputs/{dt_str}/inference_results', model_name\n",")\n","os.makedirs(infer_result_path, exist_ok=True)"],"metadata":{"id":"Dn73zdrGdpFC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"erXbOy63N-kJ"},"source":["### Utilities"]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import glob\n","\n","def annotate_image(image, output_class):\n","    image = denormalize(image).cpu()\n","    image = image.squeeze(0).permute((1, 2, 0)).numpy()\n","    image = np.ascontiguousarray(image, dtype=np.float32)\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    class_name = CLASS_NAMES[int(output_class)]\n","    plant_name = class_name.split(' ')[0]\n","    disease_name = ' '.join(class_name.split(' ')[1:])\n","\n","    rectangle_bgr = (255, 255, 255)\n","    thickness = 1\n","    alpha = 0.2  # Transparency factor.\n","\n","    font1 = cv2.FONT_HERSHEY_DUPLEX\n","    scale1 = 0.8\n","    x1, y1 = 5, 25\n","    w1, h1 = cv2.getTextSize(plant_name, font1, fontScale=scale1, thickness=thickness)[0]\n","\n","    b1_start = (x1-2, y1+2)\n","    b1_end = (x1 + w1 + 2, y1 - h1 - 2)\n","    overlay = image.copy()\n","    cv2.rectangle(overlay, b1_start, b1_end, (0, 200, 0), cv2.FILLED)\n","    image_new = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n","\n","    font2 = cv2.FONT_HERSHEY_PLAIN\n","    scale2 = 1.0\n","    x2, y2 = 5, 45\n","    w2, h2 = cv2.getTextSize(disease_name, font2, fontScale=scale2, thickness=thickness)[0]\n","\n","    b2_start = (x2-2, y2+2)\n","    b2_end = (x2 + w2 + 2, y2 - h2 - 2)\n","    overlay = image_new.copy()\n","    cv2.rectangle(overlay, b2_start, b2_end, (0, 200, 125), cv2.FILLED)\n","    image_new = cv2.addWeighted(overlay, alpha, image_new, 1 - alpha, 0)\n","\n","    cv2.putText(\n","        image_new, plant_name, (x1, y1),\n","        cv2.FONT_HERSHEY_DUPLEX, scale1, (255, 0, 0), thickness,\n","        lineType=cv2.LINE_AA\n","    )\n","\n","    cv2.putText(\n","        image_new, disease_name, (5, 45),\n","        cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 1,\n","        lineType=cv2.LINE_AA\n","    )\n","\n","    return image_new"],"metadata":{"id":"x57BWnOTdvKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWuoNSO5SaaZ"},"source":["### Transform"]},{"cell_type":"code","source":["def get_inference_transform(image_size):\n","    inference_transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((image_size, image_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","            )\n","    ])\n","    return inference_transform"],"metadata":{"id":"ZQSErqJ6dyeC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtrgNg4uR9NW"},"source":["### Inferences Function"]},{"cell_type":"code","source":["def inference(model, testloader, DEVICE):\n","    model.eval()\n","    counter = 0\n","    with torch.no_grad():\n","        counter += 1\n","        image = testloader\n","        image = image.to(DEVICE)\n","\n","        # Forward pass.\n","        outputs = model(image)\n","\n","    # Softmax probabilities.\n","    predictions = F.softmax(outputs, dim=1).cpu().numpy()\n","\n","    # Predicted class number.\n","    output_class = np.argmax(predictions)\n","\n","    # Show and save the results.\n","    result = annotate_image(image, output_class)\n","    return result"],"metadata":{"id":"ntPqM-1xd1N9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqtlrw9TOr7P"},"source":["### Load the Model"]},{"cell_type":"code","source":["load_model = build_model(model_name, fine_tune=False, dropout=dropout, num_classes=len(CLASS_NAMES)).to(DEVICE)\n","\n","load_model.load_state_dict(checkpoint['model_state_dict'])"],"metadata":{"id":"hdXXzO6Vd4OR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4OSNTgTO6Za"},"source":["### Inferences"]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","\n","inference_image_paths = glob.glob(os.path.join('/content/drive/MyDrive/Final-project/Dataset', 'inference_data', '*'))\n","\n","transform = get_inference_transform(IMAGE_SIZE)\n","\n","for i, image_path in enumerate(inference_image_paths):\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = transform(image)\n","    image = torch.unsqueeze(image, 0)\n","    result = inference(load_model, image, DEVICE)\n","\n","    # Save the image to disk.\n","    image_name = image_path.split(os.path.sep)[-1]\n","    print(f\"Image: {image_name}\")\n","    cv2_imshow(result*255.)\n","    print(f\"\\n\")\n","\n","    cv2.waitKey(1)\n","    cv2.imwrite(\n","        os.path.join(infer_result_path, image_name), result*255.\n","    )"],"metadata":{"id":"ZCJbQtind7_P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLbczSkjjmfW"},"source":["# Create Summary File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_m_gUSAIf_2R"},"outputs":[],"source":["len_train = f\"Number of training data: {len(dataset_train)}\"\n","len_val = f\"Number of validation data: {len(dataset_valid)}\"\n","len_test = f\"Number of testing data: {len(dataset_test)}\"\n","cls_count = f\"Class count: {len(dataset_classes)}\"\n","cls_names = f\"Class names: {dataset_classes}\"\n","\n","model_name = f\"Model: {model_name}\"\n","dout = f\"Dropout: {dropout}\"\n","epoch = f\"Epochs: {epochs}\"\n","lr = f\"Learning rate: {learning_rate}\"\n","opt = f\"Optimizer: {optimizer}\"\n","\n","best_epoch = f\"Best epoch: {save_best_model.best_epoch}\"\n","best_training_loss = f\"Best training loss: {save_best_model.best_train_loss:.3f}\"\n","best_training_acc = f\"Best training accuracy: {save_best_model.best_train_acc:.3f}\"\n","best_validation_loss = f\"Best validation loss: {save_best_model.best_valid_loss:.3f}\"\n","best_validation_acc = f\"Best validation accuracy: {save_best_model.best_valid_acc:.3f}\"\n","\n","testing_acc = f\"Testing accuracy: {acc:.3f}\"\n","\n","print(f'Experiment Summary\\n')\n","print(f'Date: {dt_str}\\n\\n')\n","\n","print('-'*50 + '\\n')\n","print('Dataset\\n')\n","print('-'*50 + '\\n')\n","print(f'{len_train}\\n')\n","print(f'{len_val}\\n')\n","print(f'{len_test}\\n\\n')\n","print(f'{cls_count}\\n')\n","print(f'{cls_names}\\n\\n')\n","\n","print('-'*50 + '\\n')\n","print('Training Settings\\n')\n","print('-'*50 + '\\n')\n","print(f'{model_name}\\n')\n","print(f'{dout}\\n')\n","print(f'{epoch}\\n')\n","print(f'{lr}\\n\\n')\n","print(f'{opt}\\n\\n')\n","\n","print('-'*50 + '\\n')\n","print('Training Results\\n')\n","print('-'*50 + '\\n')\n","print(f'{best_epoch}\\n')\n","print(f'{best_training_loss}\\n')\n","print(f'{best_training_acc}\\n')\n","print(f'{best_validation_loss}\\n')\n","print(f'{best_validation_acc}\\n')\n","print(f'{testing_acc}\\n')\n","\n","fname = f'/content/drive/MyDrive/Final-project/outputs/{dt_str}/{dt_str}_summary.txt'\n","\n","with open(fname, mode=\"wt\") as f:\n","    f.write(f'Experiment Summary\\n')\n","    f.write(f'Date: {dt_str}\\n\\n')\n","\n","    f.write('-'*50 + '\\n')\n","    f.write('Dataset\\n')\n","    f.write('-'*50 + '\\n')\n","    f.write(f'{len_train}\\n')\n","    f.write(f'{len_val}\\n')\n","    f.write(f'{len_test}\\n\\n')\n","    f.write(f'{cls_count}\\n')\n","    f.write(f'{cls_names}\\n\\n')\n","\n","    f.write('-'*50 + '\\n')\n","    f.write('Training Settings\\n')\n","    f.write('-'*50 + '\\n')\n","    f.write(f'{model_name}\\n')\n","    f.write(f'{dout}\\n')\n","    f.write(f'{epoch}\\n')\n","    f.write(f'{lr}\\n\\n')\n","    f.write(f'{opt}\\n\\n')\n","\n","    f.write('-'*50 + '\\n')\n","    f.write('Training Results\\n')\n","    f.write('-'*50 + '\\n')\n","    f.write(f'{best_epoch}\\n')\n","    f.write(f'{best_training_loss}\\n')\n","    f.write(f'{best_training_acc}\\n')\n","    f.write(f'{best_validation_loss}\\n')\n","    f.write(f'{best_validation_acc}\\n')\n","    f.write(f'{testing_acc}\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}